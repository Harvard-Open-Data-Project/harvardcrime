{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/neel/Git/harvardcrime/utils.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import pdfquery\n",
    "\n",
    "import incident\n",
    "reload(incident)\n",
    "from incident import Incident\n",
    "import timing\n",
    "reload(timing)\n",
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_curve(ltcurve):\n",
    "    # text can be within LTTextBoxHorizontal or LTTextLineHorizontal\n",
    "    # the problem is that these are interleaved\n",
    "    # so we select EITHER as they come\n",
    "    # if we chose the Boxes first and the Lines second then merged those lists,\n",
    "    # the resulting list would be out of order!\n",
    "    # e.g. if the true order is B1 L1 B2 B3 L2, the approach we are using\n",
    "    # gives you the right order... but choosing Boxes and Lines separately\n",
    "    # gives you B1 B2 B3 L1 L2!!!!\n",
    "    textual_elements = ltcurve.cssselect(\"LTTextBoxHorizontal, LTTextLineHorizontal\")\n",
    "    texts = [t.text.strip() for t in textual_elements]\n",
    "    \n",
    "    # remove empty lines\n",
    "    cleaned_texts = [t for t in texts if t != '']\n",
    "    \n",
    "    # PROBLEM with this approach: in rare cases some text from this falls way\n",
    "    # outside the ltcurve. Might it still be within the bounding box though?\n",
    "    \n",
    "    # UPDATE: try gathering all data\n",
    "    # for 11/28 consider these bboxes\n",
    "    #\n",
    "    # 10:08am [247.08, 80.197, 275.587, 90.18]\n",
    "    # hp laptop [6.0, 65.784, 737.868, 82.318]\n",
    "    # bounding box [3.36, 77.7, 754.86, 98.64]\n",
    "    \n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(pdf):\n",
    "    # filtering function\n",
    "    def comment_filter():\n",
    "        return this.get('word_margin',0) == \"0.1\" and this.get('x0', 0) == \"6.0\" and this.get('x1', 0) == \"737.868\"\n",
    "\n",
    "    comment_lines = pdf.pq('LTTextLineHorizontal').filter(comment_filter)\n",
    "\n",
    "    def extract_text(comment_line):\n",
    "        # sometimes text lives directly in the LTTextLine (which is passed),\n",
    "        # sometimes it lives within a LTTextBox within the LTTextLine.\n",
    "        # This standardizes that.\n",
    "        if comment_line.text != '':\n",
    "            # Contains text directly\n",
    "            return comment_line.text\n",
    "        else:\n",
    "            text_box = comment_line.find(\"LTTextBoxHorizontal\")\n",
    "\n",
    "            if text_box is not None:\n",
    "                # This is the Box inside the Line\n",
    "                return text_box.text\n",
    "\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    comments = [extract_text(line) for line in comment_lines]\n",
    "\n",
    "    # Now smoosh the lines together. Lines beginning with\n",
    "    # \"Officer\" are probably a new entry. Everything else\n",
    "    # should be lumped with the previous line.\n",
    "\n",
    "    cleaned_comments = []\n",
    "    for comment in comments:\n",
    "        if comment.startswith(\"Officer\"):\n",
    "            # proper new comment. add to the list!\n",
    "            cleaned_comments.append(comment)\n",
    "        else:\n",
    "            # it's the next line of the previous comment.\n",
    "            # add it to the previous comment.\n",
    "            # these lines usually have spaces between them already\n",
    "            # so no need to add a new one\n",
    "            cleaned_comments[-1] += comment\n",
    "\n",
    "    return cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incidents_of_pdf(pdf):\n",
    "    \"\"\"\n",
    "    Pass this function the result of a pdfquery.PDFQuery() function.\n",
    "    This will read through the pdf file and return a list of \n",
    "    Incident objects contained in there!\n",
    "    \n",
    "    Make sure the PDF is load()'ed before you pass it.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO watch out for things like 11/24/17 where there were no incidents. there's a specific tag for those.\n",
    "    \n",
    "    # so each individual report, as well as headers, is filed inside \n",
    "    # its own <LTCurve>. The text fields are inside <LTTextLineHorizontal>s and <LTTextBoxHorizontal>s\n",
    "    # inside the <LTCurve>.\n",
    "    reports_plus_heads = pdf.tree.findall(\".//LTCurve\")\n",
    "    \n",
    "    # extract raw incidents\n",
    "    raw_incidents = [get_text_from_curve(lt) for lt in reports_plus_heads]\n",
    "\n",
    "    # remove headers of tables\n",
    "    HEADER_ROW_TEXT = ['Reported', 'Incident Type', 'Occurred', 'Location', 'Disposition']\n",
    "    incidents_without_headers = [i for i in raw_incidents if i != HEADER_ROW_TEXT]\n",
    "    \n",
    "    # convert incidents to proper objects\n",
    "    # 9 = proper length of report; anything less is malformed\n",
    "    # TODO clean up â€” extract error checking into its own make_incident_objects() function\n",
    "    incident_objects = [incident.Incident(i) for i in incidents_without_headers if len(i) == 9]\n",
    "    \n",
    "    # get the comments from this pdf\n",
    "    comments = get_comments(pdf)\n",
    "    # there should be as many comments as incidents, so attach one to each\n",
    "    # (in order)\n",
    "    if len(comments) == len(incident_objects):\n",
    "        for i in range(0, len(comments)):\n",
    "            incident_objects[i].comments = comments[i]\n",
    "    else:\n",
    "        # PROBLEM: if the number of comments != the number of incidents,\n",
    "        # we currently don't attach any comments to anyone.\n",
    "        # is there a way to match comments individually to incidents\n",
    "        # (like the LTCurves)? perhaps by x/y coords? that might help\n",
    "        print(\"Wrong length comments!\")\n",
    "        print(comments)\n",
    "        print(len(comments))\n",
    "        print(incident_objects)\n",
    "        print(len(incident_objects))\n",
    "    \n",
    "    return incident_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong length comments!\n",
      "['Officer dispatched to take a report of a stolen U.S. currency valued at $620.00. ', 'Officer dispatched to take a report of a stolen blue Canada Goose jacket valued at $1,000.00. ', 'Officer dispatched to take a report of a stolen green Subaru Forrester valued at $12,000.00. ', 'Officer dispatched to take a report of threats. ', 'Officer dispatched to a report of an unwanted guest sitting in on a class. Officer arrived, located individual and conducted a field interview. The individual was run for wants/warrants with negative results. The individual was then advised that they need to get permission to sit in on future classes. The individual was then sent on their way. ', 'Officer dispatched to take a report of a stolen package containing an HP laptop valued at $821.00. ', 'Officers dispatched to a report of an individual wandering in and out of the library for the last 20 minutes. Officers arrived and report individual gone on arrival. ', 'Officer dispatched to a report of two individuals wandering around the lobby area of the building. Officer arrived and report individuals gone on arrival. ']\n",
      "8\n",
      "[<incident.Incident object at 0x108b37c88>, <incident.Incident object at 0x1080566d8>, <incident.Incident object at 0x108056748>, <incident.Incident object at 0x1080565c0>, <incident.Incident object at 0x108056828>, <incident.Incident object at 0x1080567f0>, <incident.Incident object at 0x108056898>]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "pdf = pdfquery.PDFQuery(\"data/112817.pdf\")\n",
    "pdf.load()\n",
    "all_incidents = incidents_of_pdf(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another date\n",
    "pdf = pdfquery.PDFQuery(\"data/113017.pdf\")\n",
    "pdf.load()\n",
    "new_incidents = incidents_of_pdf(pdf)\n",
    "all_incidents += new_incidents\n",
    "utils.dump_csv(all_incidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMAINING TODOS:\n",
    "# - have a function to programmatically download HUPD crime logs\n",
    "# - have another function to run through all downloaded crime logs in the `data` folder\n",
    "#   (requires us to read the file system?)\n",
    "# - Extract the descriptive test along with the metadata. This is somewhat harder but still\n",
    "#   very important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done data/012018.pdf\n",
      "Done data/010418.pdf\n",
      "Done data/020518.pdf\n",
      "Done data/110817.pdf\n",
      "Done data/020718.pdf\n",
      "Done data/010618.pdf\n",
      "Done data/012218.pdf\n",
      "Done data/112817.pdf\n",
      "Done data/122917.pdf\n",
      "Done data/012618.pdf\n",
      "Done data/123017.pdf\n",
      "Done data/010218.pdf\n",
      "Done data/020318.pdf\n",
      "Done data/020118.pdf\n",
      "Done data/011918.pdf\n",
      "Done data/012418.pdf\n",
      "Done data/020618.pdf\n",
      "Done data/010718.pdf\n",
      "Done data/012118.pdf\n",
      "Done data/010518.pdf\n",
      "Done data/020418.pdf\n",
      "Done data/010118.pdf\n",
      "Done data/012518.pdf\n",
      "Done data/011818.pdf\n",
      "Done data/012718.pdf\n",
      "Done data/122817.pdf\n",
      "Done data/010318.pdf\n",
      "Done data/020218.pdf\n",
      "Done data/113017.pdf\n",
      "Done data/123117.pdf\n",
      "Done data/122417.pdf\n",
      "Done data/121917.pdf\n",
      "Done data/011618.pdf\n",
      "Done data/011418.pdf\n",
      "Done data/012918.pdf\n",
      "Done data/122617.pdf\n",
      "Done data/013018.pdf\n",
      "Done data/010918.pdf\n",
      "Done data/020818.pdf\n",
      "Done data/011018.pdf\n",
      "Done data/021118.pdf\n",
      "Done data/122217.pdf\n",
      "Done data/122017.pdf\n",
      "Done data/021318.pdf\n",
      "Done data/011218.pdf\n",
      "Done data/122717.pdf\n",
      "Done data/012818.pdf\n",
      "Done data/021418.pdf\n",
      "Done data/011518.pdf\n",
      "Done data/013118.pdf\n",
      "Done data/011718.pdf\n",
      "Done data/121817.pdf\n",
      "Done data/112417.pdf\n",
      "Done data/122517.pdf\n",
      "Done data/021218.pdf\n",
      "Done data/011318.pdf\n",
      "Done data/122117.pdf\n",
      "Done data/010818.pdf\n",
      "Done data/020918.pdf\n",
      "Done data/122317.pdf\n",
      "Done data/011118.pdf\n",
      "Done data/021018.pdf\n"
     ]
    }
   ],
   "source": [
    "# Go through EVERY pdf in our data folder!\n",
    "import glob\n",
    "\n",
    "all_incidents = []\n",
    "\n",
    "for filename in glob.iglob('data/*.pdf'):\n",
    "    # filename will be like `data/xxxxxx.pdf`\n",
    "    # extract incidents from this file\n",
    "    pdf = pdfquery.PDFQuery(filename)\n",
    "    pdf.load()\n",
    "    new_incidents = incidents_of_pdf(pdf)\n",
    "    all_incidents += new_incidents\n",
    "    \n",
    "    print(\"Done {}\".format(filename))\n",
    "    \n",
    "# dump to csv\n",
    "utils.dump_csv(all_incidents)\n",
    "print(\"Dumped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "crime",
   "language": "python",
   "name": "crime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
